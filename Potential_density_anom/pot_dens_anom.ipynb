{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29612233",
   "metadata": {},
   "source": [
    "## Havvind - Integrated potential density anomaly at Sørvest-F and Nordvest-C\n",
    "\n",
    "Potential density have been calculated for all daily reference and experiment files. Files are found in: `/lustre/storeB/project/nwp/havvind/hav/analysis_kjsta/output_mld`, with output variable named `pd`. This is the total potential density, not anomaly. It was calculated by interpolating potential temperature and salinity from s-levels to z-levels.\n",
    "\n",
    "Potential density anomaly is already an output variable in the reference and experiment files, however they are on s-levels. As I want to integrated over depth and save some time, I will use the already interpolated potential density.\n",
    "\n",
    "I think the integrated potential density anomaly is the same as the potential energy anomaly? In https://doi.org/10.1016/j.ocemod.2007.12.003 it is defined as \n",
    "$$\\varphi = \\frac{1}{H} \\int_{-h}^{\\eta}(\\bar{\\rho} - \\rho) gz dz$$\n",
    "\n",
    "\"...where ρ is the vertical density profile over the water column of depth H, given by H = η + h, η the free surface, h the location of the bed, $\\bar\\rho$\n",
    " the depth averaged density, z the vertical co-ordinate and g the gravitational acceleration. For a given density profile, φ (J/m3) represents the amount of work required to bring about complete vertical mixing per unit of volume.\"\n",
    "\n",
    " From https://doi.org/10.3389/fmars.2025.1531684 we have the same definition, and they say \"The potential energy anomaly (ϕ) and its derivative (ϕ_t) are used to evaluate the competition between mixing and stratification in the water column and the processes that cause it. ϕ explains the amount of mechanical energy (per m3) required to reach a specific density profile in the water column with a given density profile. \"\n",
    "\n",
    " Okay my thoughts:\n",
    "\n",
    " My first immediate thought was to do $$\\int_{-H}^{0} \\rho' dz$$, but that doesn't really make much sense I think. Because what I think we want to know is if the turbines and changed wind stress alters mixing. If we look at the potential energy however, we get a feel for the mixing. Higher potential energy would be more mixed and vice versa. Meaning we would go with the equation found in the other papers above. Because if there is more or less mixing in the presence of wind farms, a change in potential energy of the column would be seen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13c0aa",
   "metadata": {},
   "source": [
    "Starting with making a grid, opening turbine coordinates, extracting the files containing potential density, calculating the Rossby radius and making a study area of $50R_1$ x $50R_1$ around the wind farms (where the Rossby deformation radius is the area mean of June), and getting out the z-levels which the data was interpolated to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb9245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from Rossby_deformation.get_turbine_coords import get_turbine_coords\n",
    "from Rossby_deformation.funcs import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as mplstyle\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "mplstyle.use(['ggplot', 'fast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe2d101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a grid\n",
    "path = '/lustre/storeB/project/nwp/havvind/hav/results/experiment/EXP-03/norkyst_avg_0001.nc'\n",
    "fid = Dataset(path)\n",
    "grid = SGrid(fid)\n",
    "del fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c4e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the turbine coordinates as a xArray DataSet\n",
    "sorvest_F = get_turbine_coords('/lustre/storeB/project/nwp/havvind/turbine_coordinates/windfarms_Sorvest_F.txt')\n",
    "nordvest_C = get_turbine_coords('/lustre/storeB/project/nwp/havvind/turbine_coordinates/windfarms_Nordvest_C.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7591acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square around wind park\n",
    "min_lon_SV = np.min(sorvest_F.coordinates[:,0].values)\n",
    "min_lat_SV = np.min(sorvest_F.coordinates[:,1].values)\n",
    "max_lon_SV = np.max(sorvest_F.coordinates[:,0].values)\n",
    "max_lat_SV = np.max(sorvest_F.coordinates[:,1].values)\n",
    "\n",
    "area_lon_SV = [min_lon_SV, max_lon_SV]\n",
    "area_lat_SV = [min_lat_SV, max_lat_SV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f8175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square around wind park\n",
    "min_lon_NV = np.min(nordvest_C.coordinates[:,0].values)\n",
    "min_lat_NV = np.min(nordvest_C.coordinates[:,1].values)\n",
    "max_lon_NV = np.max(nordvest_C.coordinates[:,0].values)\n",
    "max_lat_NV = np.max(nordvest_C.coordinates[:,1].values)\n",
    "\n",
    "area_lon_NV = [min_lon_NV, max_lon_NV]\n",
    "area_lat_NV = [min_lat_NV, max_lat_NV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2191461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting paths to files containing the computed Rossby deformation radius\n",
    "# Note: I'm using the reference datasets and not the experiments\n",
    "# Rossby deformation radius computed from the experiment datasets are found in output_bdr/EXP\n",
    "\n",
    "filefolder = glob('/lustre/storeB/project/nwp/havvind/hav/analysis_kjsta/output_bdr/REF')\n",
    "\n",
    "# Only using June because we want the largest Rossby radius\n",
    "months = {\n",
    "\"06\": 30   # June\n",
    "}\n",
    "\n",
    "files=[]  # empty list to store paths in\n",
    "\n",
    "# building paths to contain each daily file and named thereafter\n",
    "for month, days in months.items():\n",
    "    for day in range(1, days + 1): \n",
    "        day_str = f\"{day:04}\"\n",
    "        file_path = f'/REF_{month}_norkyst_avg_{day_str}_brr.nc'\n",
    "        files.append(filefolder[0]+file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd9d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal Rossby radius of June - from area of wind parks\n",
    "R1_june_SV = monthly_mean_area(files, grid, area_lon_SV, area_lat_SV)\n",
    "R1_june_SV = R1_june_SV.gamma_r\n",
    "\n",
    "# Internal Rossby radius of June - from area of wind parks\n",
    "R1_june_NV = monthly_mean_area(files, grid, area_lon_NV, area_lat_NV)\n",
    "R1_june_NV = R1_june_NV.gamma_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5865f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zlevs = np.arange(0,51,1)\n",
    "zlevs = np.insert(zlevs,len(zlevs),values=np.arange(52,102,2), axis =0)\n",
    "zlevs = np.insert(zlevs,len(zlevs),values=np.arange(105,305,5), axis =0)\n",
    "zlevs = np.insert(zlevs,len(zlevs),values=np.arange(520,1020,20), axis =0)\n",
    "zlevs = np.insert(zlevs,len(zlevs),values=np.arange(1050,3050,50), axis =0)\n",
    "\n",
    "zlevs = zlevs[np.where(zlevs<=np.max(grid.h))]\n",
    "zlevs = np.array(zlevs)*-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7635ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting filepaths to MLD for the reference runs\n",
    "\n",
    "filefolder = glob('/lustre/storeB/project/nwp/havvind/hav/analysis_kjsta/output_mld/REF')\n",
    "\n",
    "months = {\n",
    "\"02\": 27,  # February\n",
    "\"03\": 31,  # March\n",
    "\"04\": 30,  # April\n",
    "\"05\": 31,  # May\n",
    "\"06\": 30   # June\n",
    "}\n",
    "\n",
    "files_ref=[]  # empty list to store paths in\n",
    "\n",
    "# building paths to contain each daily file and named thereafter\n",
    "for month, days in months.items():\n",
    "    for day in range(1, days + 1): \n",
    "        day_str = f\"{day:04}\"\n",
    "        file_path = f'/REF_{month}_norkyst_avg_{day_str}_mld.nc'\n",
    "        files_ref.append(filefolder[0]+file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b601d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting filepaths to MLD for the experiments\n",
    "\n",
    "filefolder = glob('/lustre/storeB/project/nwp/havvind/hav/analysis_kjsta/output_mld/EXP')\n",
    "\n",
    "months = {\n",
    "\"02\": 27,  # February\n",
    "\"03\": 31,  # March\n",
    "\"04\": 30,  # April\n",
    "\"05\": 31,  # May\n",
    "\"06\": 30   # June\n",
    "}\n",
    "\n",
    "files_exp=[]  # empty list to store paths in\n",
    "\n",
    "# building paths to contain each daily file and named thereafter\n",
    "for month, days in months.items():\n",
    "    for day in range(1, days + 1): \n",
    "        day_str = f\"{day:04}\"\n",
    "        file_path = f'/EXP_{month}_norkyst_avg_{day_str}_mld.nc'\n",
    "        files_exp.append(filefolder[0]+file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "913e3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLD and potential dens at Sørvest-F - study area is 60R1x60R1 around wind farm.\n",
    "# Area lon/lat is the extent of the study area\n",
    "lon_SV, lat_SV, r_SV = make_study_area(files_ref, grid, area_lon_SV, area_lat_SV, R1_june_SV)\n",
    "lon_SV, lat_SV, e_SV = make_study_area(files_exp, grid, area_lon_SV, area_lat_SV, R1_june_SV)\n",
    "\n",
    "# MLD and potential dens at Sørvest-F - study area is 60R1x60R1 around wind farm.\n",
    "# Area lon/lat is the extent of the study area\n",
    "lon_NV, lat_NV, r_NV = make_study_area(files_ref, grid, area_lon_NV, area_lat_NV, R1_june_NV)\n",
    "lon_NV, lat_NV, e_NV = make_study_area(files_exp, grid, area_lon_NV, area_lat_NV, R1_june_NV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f70525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the zlevs as a dimension\n",
    "ref_SV = r_SV.expand_dims(dim={'depth': zlevs})\n",
    "exp_SV = e_SV.expand_dims(dim={'depth': zlevs})\n",
    "\n",
    "ref_NV = r_NV.expand_dims(dim={'depth': zlevs})\n",
    "exp_NV = e_NV.expand_dims(dim={'depth': zlevs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3bf835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving some memory\n",
    "del r_SV\n",
    "del e_SV\n",
    "del r_NV\n",
    "del e_NV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a8d57",
   "metadata": {},
   "source": [
    "Now that we have that, we have to convert the potential density to potential density anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f422c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_anom_ref_SV = ref_SV.pd - 1000\n",
    "pd_anom_exp_SV = exp_SV.pd - 1000\n",
    "\n",
    "pd_anom_ref_NV = ref_NV.pd - 1000\n",
    "pd_anom_exp_NV = exp_NV.pd - 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31675b48",
   "metadata": {},
   "source": [
    "Testing out how to do this on one daily file for starters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80a0d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting filepaths to the reference runs\n",
    "\n",
    "filefolder = glob('/lustre/storeB/project/nwp/havvind/hav/results/reference/')\n",
    "\n",
    "months = {\n",
    "\"02\": 27,  # February\n",
    "\"03\": 31,  # March\n",
    "\"04\": 30,  # April\n",
    "\"05\": 31,  # May\n",
    "\"06\": 30   # June\n",
    "}\n",
    "\n",
    "files_ref=[]  # empty list to store paths in\n",
    "\n",
    "# building paths to contain each daily file and named thereafter\n",
    "for month, days in months.items():\n",
    "    for day in range(1, days + 1): \n",
    "        day_str = f\"{day:04}\"\n",
    "        file_path = f'/REF-{month}/norkyst_avg_{day_str}.nc'\n",
    "        files_ref.append(filefolder[0]+file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aa73e2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m day_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mday\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/EXP-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/norkyst_avg_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mday_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 20\u001b[0m files_exp\u001b[38;5;241m.\u001b[39mappend(filefolder[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mfile_path)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Extracting filepaths to the experiment runs\n",
    "\n",
    "filefolder = glob('/lustre/storeB/project/nwp/havvind/hav/results/experiments/')\n",
    "\n",
    "months = {\n",
    "\"02\": 27,  # February\n",
    "\"03\": 31,  # March\n",
    "\"04\": 30,  # April\n",
    "\"05\": 31,  # May\n",
    "\"06\": 30   # June\n",
    "}\n",
    "\n",
    "files_exp=[]  # empty list to store paths in\n",
    "\n",
    "# building paths to contain each daily file and named thereafter\n",
    "for month, days in months.items():\n",
    "    for day in range(1, days + 1): \n",
    "        day_str = f\"{day:04}\"\n",
    "        file_path = f'/EXP-{month}/norkyst_avg_{day_str}.nc'\n",
    "        files_exp.append(filefolder[0]+file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55498351",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ref = xr.open_dataset('/lustre/storeB/project/nwp/havvind/hav/results/experiment/EXP-03/norkyst_avg_0001.nc')\n",
    "zeta = ds_ref.zeta\n",
    "del ds_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ref = xr.open_dataset('/lustre/storeB/project/nwp/havvind/hav/analysis_kjsta/output_mld/EXP/EXP_03_norkyst_avg_0001_mld.nc')\n",
    "ds_ref = ds_ref.expand_dims(dim={'depth': zlevs})\n",
    "pd = ds_ref.pd\n",
    "del ds_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b2c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = grid.h\n",
    "mask = grid.mask_rho\n",
    "z_r = grid.z_r\n",
    "g = 9.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf01ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrating N over z-levels\n",
    "                for k in range(0, len(N) - 1):\n",
    "                    tmpn[0, y, x] = tmpn[0, y, x] + np.abs(\n",
    "                        N_depth[k] - N_depth[k + 1]\n",
    "                    ) / 2.0 * (N[k] + N[k + 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea90afc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [48], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m h \u001b[38;5;241m=\u001b[39m zlevs[np\u001b[38;5;241m.\u001b[39mwhere(zlevs[:] \u001b[38;5;241m>\u001b[39m z_r[:, y, x]\u001b[38;5;241m.\u001b[39mmin())]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m----> 6\u001b[0m n \u001b[38;5;241m=\u001b[39m zeta[\u001b[38;5;241m0\u001b[39m, y, x]\n",
      "File \u001b[0;32m/modules/rhel8/conda/install/envs/production-10-2022/lib/python3.9/site-packages/xarray/core/dataarray.py:772\u001b[0m, in \u001b[0;36mDataArray.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_coord(key)\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;66;03m# xarray-style array indexing\u001b[39;00m\n\u001b[0;32m--> 772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_item_key_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/modules/rhel8/conda/install/envs/production-10-2022/lib/python3.9/site-packages/xarray/core/dataarray.py:1291\u001b[0m, in \u001b[0;36mDataArray.isel\u001b[0;34m(self, indexers, drop, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# Much faster algorithm for when all indexers are ints, slices, one-dimensional\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;66;03m# lists, or zero or one-dimensional np.ndarray's\u001b[39;00m\n\u001b[0;32m-> 1291\u001b[0m variable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1292\u001b[0m indexes, index_variables \u001b[38;5;241m=\u001b[39m isel_indexes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxindexes, indexers)\n\u001b[1;32m   1294\u001b[0m coords \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/modules/rhel8/conda/install/envs/production-10-2022/lib/python3.9/site-packages/xarray/core/variable.py:1226\u001b[0m, in \u001b[0;36mVariable.isel\u001b[0;34m(self, indexers, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   1223\u001b[0m indexers \u001b[38;5;241m=\u001b[39m drop_dims_from_indexers(indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims, missing_dims)\n\u001b[1;32m   1225\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(indexers\u001b[38;5;241m.\u001b[39mget(dim, \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims)\n\u001b[0;32m-> 1226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/modules/rhel8/conda/install/envs/production-10-2022/lib/python3.9/site-packages/xarray/core/variable.py:789\u001b[0m, in \u001b[0;36mVariable.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_order:\n\u001b[1;32m    788\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmoveaxis(data, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(new_order)), new_order)\n\u001b[0;32m--> 789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finalize_indexing_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/modules/rhel8/conda/install/envs/production-10-2022/lib/python3.9/site-packages/xarray/core/variable.py:793\u001b[0m, in \u001b[0;36mVariable._finalize_indexing_result\u001b[0;34m(self, dims, data)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_finalize_indexing_result\u001b[39m(\u001b[38;5;28mself\u001b[39m: T_Variable, dims, data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_Variable:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;124;03m\"\"\"Used by IndexVariable to return IndexVariable objects when possible.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/modules/rhel8/conda/install/envs/production-10-2022/lib/python3.9/site-packages/xarray/core/variable.py:984\u001b[0m, in \u001b[0;36mVariable._replace\u001b[0;34m(self, dims, data, attrs, encoding)\u001b[0m\n\u001b[1;32m    982\u001b[0m     attrs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs)\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m _default:\n\u001b[0;32m--> 984\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(dims, data, attrs, encoding, fastpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/modules/rhel8/conda/install/envs/production-10-2022/lib/python3.9/copy.py:76\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     74\u001b[0m copier \u001b[38;5;241m=\u001b[39m _copy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# treat it as a regular class:\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _copy_immutable(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pot_energy = np.ones((1, pd.shape[3], pd.shape[4]))*np.nan\n",
    "pot_energy[0] = 1\n",
    "\n",
    "for y in range(pd.shape[3]):\n",
    "    for x in range(pd.shape[4]):\n",
    "        if not mask[y, x]:\n",
    "            continue\n",
    "\n",
    "        tmpz = zlevs[np.where(zlevs[:] > z_r[:, y, x].min())].squeeze()\n",
    "        z_mid = (tmpz[0:-1] + tmpz[1:]) / 2\n",
    "\n",
    "        n = zeta[0, y, x]\n",
    "        h = depth[y, x]\n",
    "        H = n + h\n",
    "        rho_mean = pd[:, 0, 0, y, x].mean(dim='depth')\n",
    "        rho = pd[:, 0, 0, y, x]\n",
    "        rho_diff = rho_mean - rho\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:production-10-2022] *",
   "language": "python",
   "name": "conda-env-production-10-2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
